\documentclass[FinalReport.tex]{subfiles}
\begin{document}

With short memory, control algorithm proposed in \cite{OptCont} achieves to keep the time series bounded, but yields power law distributed fluctuations, questioning the term "optimal control". Indeed, optimality supposedly comes from the Maximum Likelihood Estimation of $\alpha_0$. If the time series on which parameter is estimated is not long enough, MLE properties breaks down. When fluctuations get close to the origin information about the dynamics gets lost into noise (recall the variance of the estimator $\sigma_0^2/n^2 \hat{\sigma}_z^2$). In the extreme case $n=1$ the estimator is per se divergent, because of the inverse factor $z_{t-1}^{-1}$. 
Recasting the process in Kesten form demonstrates that this model intrinsically produces power laws.
With more memory however tail events have more probability to be captured by the estimator and averaged out, thanks to the convergence of tail distribution towards a normal distribution, which requires more samples than the centre. With this first part we therefore describe in details the properties of the initial model, and explain the convergence towards optimal control as memory increases. This results contradicts the claim that criticality originates from optimal control itself. Extreme fluctuations simply come from a naive use of MLE. As the model fits well with experimental data \cite{FrontNanoScience}, understanding deeply its characteristics gives interesting insights on how human performs visuomotor closed-loop control. Besides, the same underlying stochastic process was also found in describing synaptic size dynamics \cite{Holler2021}. 

Considering the very short memory case ($n=1$), we propose a modification that prevents divergence of the estimator: When the ratio of two consecutive deviations gets larger than some threshold, MLE estimator is replaced by a constant increment in the descent direction. This replacement is still adaptive as this direction is obtained from previous observations. Numerical simulations show that this naive modification leads to optimal control for well-chosen parameters. In particular, the threshold must be small enough to ensure that the original estimator is replaced sufficiently often. Otherwise, former power law behavior dominates the distribution and results are the same as before. Without knowledge of hidden parameters $\alpha_0$ and $\sigma_0^2$, the choice of parameters is not straightforward. Hopefully, simulations show that using the same parameters for both the threshold and the increment yields good results, reducing the complexity of the model. Another choice with satisfying results is to set the threshold to zero, that is getting rid of the original model. One major inconvenient of this modification is that for certain initial conditions and parameters the dynamics can be stuck in an unstable regime, before reaching stationarity. Knowing $\alpha_0$ and $\sigma_0^2$ it is possible to numerically optimize the parameters, by minimizing KS statistics or sample variance. When possible, the best would thus be to use a training set to optimize $\eta$ before using the controller. 


Despite its optimality, our proposed modification is indeed not of practical use in the context of the initial model. More generally, strategies of adaptive control in engineering also often rely on parameter estimation \cite{adapt-control}, but are more complex. MLE is not commonly use, as it requires large datasets. Kalman filters are for example widely used to denoise past observations, and could have been a great alternative to our naive modification.


 

\end{document}